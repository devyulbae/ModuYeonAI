{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devyulbae/AIClass/blob/main/Q8_Text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcD2nPQvPOFM"
      },
      "source": [
        "# Text Generation with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpaIgZGRrSaw"
      },
      "source": [
        "* RNN을 사용하여 어떻게 텍스트를 생성하는지 알아보자.\n",
        "* 주어진 텍스트를 기반으로 텍스트를 생성하는 모델을 구현해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "### 생성의 한계\n",
        "\n",
        "문장 중 일부는 문법적으로 맞지만 대부분 자연스럽지 않다.\n",
        "\n",
        "이 모델은 단어의 의미를 학습하지는 않았지만, 고려해야 할 점으로:\n",
        "\n",
        "* 데이터는 문자 기반이다. 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르고 심지어 텍스트의 단위가 단어라는 것도 모른다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46TMSZVszoc9"
      },
      "source": [
        "### Drive 연결\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiWZ-S31zokw"
      },
      "source": [
        "use_colab = True\n",
        "assert use_colab in [True, False]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjWuSiewzw-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9208a3c9-dc5e-4eef-e611-46b16fe1f774"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### 셰익스피어 데이터셋 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### 데이터 읽기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d9cdf3-841d-4914-a027-ea2604ff191a"
      },
      "source": [
        "# 데이터를 불러와서 디코딩\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 문자의 수\n",
        "print ('텍스트의 길이: {}'.format(len(text)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "텍스트의 길이: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d1c86e-487b-4f2d-8172-a7d6cd9dbe10"
      },
      "source": [
        "# 텍스트 처음 250자 출력\n",
        "print(text[:250])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53a778a-d7e7-4d03-81bc-9da8586dbe1b"
      },
      "source": [
        "# 파일의 고유 문자수를 출력\n",
        "vocab = sorted(set(text)) # 내가 불러온 text 데이터를 집합으로 만들어서 정렬시킨 상태입니다.\n",
        "print ('고유 문자수 {}개'.format(len(vocab)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "고유 문자수 65개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## 텍스트 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### 텍스트 벡터화\n",
        "\n",
        "* 학습을 위해서 텍스트들을 수치화할 필요가 있다.\n",
        "* 텍스트를 인덱스화 시켜 학습에 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "# 고유 문자에서 인덱스로 매핑 생성\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab) # idx <=> char 변환할 수 있는 사전을 들고있는 것이 중요합니다.\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text]) # 람다식"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPQ2A1mwOQUd",
        "outputId": "21f283bd-ef0c-43ff-fe91-ac37158c32db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "idx2char"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
              "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
              "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
              "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
              "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
              "      dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdVhhjYa31Uk",
        "outputId": "2bdfe1a5-1d91-4c1f-9061-8630f10589b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_as_int)\n",
        "text_as_int.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18 47 56 ... 45  8  0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "* 텍스트 0번부터 전체 텍스트 길이까지 인덱스화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYyNlCNXymwY",
        "outputId": "1277b801-459a-4122-e97e-628423550f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(65)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  'H' :  20,\n",
            "  'I' :  21,\n",
            "  'J' :  22,\n",
            "  'K' :  23,\n",
            "  'L' :  24,\n",
            "  'M' :  25,\n",
            "  'N' :  26,\n",
            "  'O' :  27,\n",
            "  'P' :  28,\n",
            "  'Q' :  29,\n",
            "  'R' :  30,\n",
            "  'S' :  31,\n",
            "  'T' :  32,\n",
            "  'U' :  33,\n",
            "  'V' :  34,\n",
            "  'W' :  35,\n",
            "  'X' :  36,\n",
            "  'Y' :  37,\n",
            "  'Z' :  38,\n",
            "  'a' :  39,\n",
            "  'b' :  40,\n",
            "  'c' :  41,\n",
            "  'd' :  42,\n",
            "  'e' :  43,\n",
            "  'f' :  44,\n",
            "  'g' :  45,\n",
            "  'h' :  46,\n",
            "  'i' :  47,\n",
            "  'j' :  48,\n",
            "  'k' :  49,\n",
            "  'l' :  50,\n",
            "  'm' :  51,\n",
            "  'n' :  52,\n",
            "  'o' :  53,\n",
            "  'p' :  54,\n",
            "  'q' :  55,\n",
            "  'r' :  56,\n",
            "  's' :  57,\n",
            "  't' :  58,\n",
            "  'u' :  59,\n",
            "  'v' :  60,\n",
            "  'w' :  61,\n",
            "  'x' :  62,\n",
            "  'y' :  63,\n",
            "  'z' :  64,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1VKcQHcymwb",
        "outputId": "6a13b6a4-2c2d-4669-8e4f-b6f9593ef5b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 텍스트 맵핑\n",
        "print ('{} ---- Index ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen' ---- Index ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### 예측 과정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "주어진 문자나 문자 시퀀스가 주어졌을 때, 다음 문자로 가장 가능성 있는 문자는 무엇일까?\n",
        "\n",
        "* 이는 모델을 훈련하여 수행할 작업이다.\n",
        "* 모델의 입력은 문자열 시퀀스가 될 것이고, 모델을 훈련시켜 출력을 예측한다.\n",
        "* 이 출력은 현재 타임 스텝(time step)의 다음 문자이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### 훈련 샘플과 타깃 만들기\n",
        "\n",
        "* 다음으로 텍스트를 샘플 시퀀스로 나누자.\n",
        "\n",
        "* 각 입력 시퀀스에는 텍스트에서 나온 `seq_length`개의 문자가 포함된다.\n",
        "\n",
        "* 각 입력 시퀀스에서, 해당 타깃은 한 문자를 오른쪽으로 이동한 것을 제외하고는 동일한 길이의 텍스트를 포함한다.\n",
        "\n",
        "* 텍스트를`seq_length + 1`개의 청크(chunk)로 나누자\n",
        "    * 예를 들어, `seq_length`는 4이고 텍스트를 \"Hello\"이라고 가정해 봅시다. 입력 시퀀스는 \"Hell\"이고 타깃 시퀀스는 \"ello\"가 된다.\n",
        "\n",
        "* 이렇게 하기 위해 먼저 `tf.data.Dataset.from_tensor_slices` 함수를 사용해 텍스트 벡터를 문자 인덱스의 스트림으로 변환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjXwjMSQbNnD"
      },
      "source": [
        "# Hello 라는 단어를 만들고 싶습니다.\n",
        "# 학습 데이터를 아래와 같이 세팅을 해줘야합니다.\n",
        "# input : H -> e -> l -> l\n",
        "# output : e -> l -> l -> o"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHJDA39zf-O",
        "outputId": "4c265bd0-c9dc-4ea3-d3e9-60a6c6293aec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 훈련 샘플/타깃 만들기\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # 알파벳을 하나씩 생성합니다.\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(i.numpy())\n",
        "    print(idx2char[i.numpy()])#확인가능"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "F\n",
            "47\n",
            "i\n",
            "56\n",
            "r\n",
            "57\n",
            "s\n",
            "58\n",
            "t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "* `batch`를 이용해 몇개의 텍스트를 가져올 것인지 정할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4hkDU3i7ozi",
        "outputId": "2a9141b6-2d0b-4f49-9836-774b0fb92490",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length +1, drop_remainder=True) # 문장을 가져와합니다.\n",
        "                               # 문장을 가져올 수 있도록 batch 를 구성해줍니다.\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()]))) # repr 개행문자 출력"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "각 시퀀스에서, `map` 메서드를 사용해 각 배치에 간단한 함수를 적용하고 입력 텍스트와 타깃 텍스트를 복사 및 이동\n",
        "* 모델이 Text를 생성할때, 한 텍스트 단위로 생성하기 때문에 그 다음 텍스트를 target으로 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "# Hello -> 불러온 데이터의 길이\n",
        "# input : Hell # input의 길이\n",
        "# output : ello # output의 길이\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCopyGZymwi"
      },
      "source": [
        "첫 번째 샘플의 타깃 값을 출력해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "outputId": "82416828-1539-469e-9292-9a2c39403d84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 데이터:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "타깃 데이터:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33OHL3b84i0"
      },
      "source": [
        "* 이 벡터의 각 인덱스는 하나의 타임 스텝(time step)으로 처리됩니다. 타임 스텝 0의 입력으로 모델은 \"F\"의 인덱스를 받고 다음 문자로 \"i\"의 인덱스를 예측한다.\n",
        "\n",
        "* 다음 타임 스텝에서도 같은 일을 하지만 RNN은 현재 입력 문자 외에 이전 타임 스텝의 컨텍스트**(context)**를 고려한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eBu9WZG84i0",
        "outputId": "58c762a0-a622-4d00-e9fc-b286506d192c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"iter {:4d}\".format(i))\n",
        "    print(\"  inputs: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  generated text: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter    0\n",
            "  inputs: 18 ('F')\n",
            "  generated text: 47 ('i')\n",
            "iter    1\n",
            "  inputs: 47 ('i')\n",
            "  generated text: 56 ('r')\n",
            "iter    2\n",
            "  inputs: 56 ('r')\n",
            "  generated text: 57 ('s')\n",
            "iter    3\n",
            "  inputs: 57 ('s')\n",
            "  generated text: 58 ('t')\n",
            "iter    4\n",
            "  inputs: 58 ('t')\n",
            "  generated text: 1 (' ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### 훈련 배치 생성\n",
        "\n",
        "* 텍스트를 다루기 쉬운 시퀀스로 분리하기 위해 `tf.data`를 사용\n",
        "* 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치를 만들어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S",
        "outputId": "dcaf9df5-817a-4392-ac49-b75bba2e243a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 배치 크기\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset\n",
        "# 알파벳을 기준으로 했던 데이터에서 문장단위로 데이터를 불러오는 dataset을 구성할 수 있습니다."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "모델을 정의하려면 `tf.keras.Sequential`을 사용한다.\n",
        "\n",
        "이 예제에서는 3개의 층을 사용하여 모델을 정의한다:\n",
        "\n",
        "* `tf.keras.layers.Embedding` : 입력층. `embedding_dim` 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련 가능한 검색 테이블.\n",
        "* `tf.keras.layers.GRU` : 크기가 `units = rnn_units`인 RNN의 유형(여기서 LSTM층을 사용할 수도 있다.)\n",
        "* `tf.keras.layers.Dense` : 크기가 `vocab_size`인 출력을 생성하는 출력층.\n",
        "\n",
        "각 문자에 대해 모델은 임베딩을 검색하고, 임베딩을 입력으로 하여 GRU를 1개의 타임 스텝으로 실행하고, FC layers를 적용하여 다음 문자의 로그 가능도(log-likelihood)를 예측하는 로짓을 생성한다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# 문자로 된 어휘 사전의 크기\n",
        "vocab_size = len(char2idx)\n",
        "\n",
        "# 임베딩 차원\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN 유닛(unit) 개수\n",
        "rnn_units = 1024"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.GRU(units=rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgzQLegT2H_J"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbNnqjvhI9lk",
        "outputId": "ba15729f-7ff7-487f-d714-0d697ab4e5e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.output_shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, None, 256)\n",
            "(64, None, 1024)\n",
            "(64, None, 1024)\n",
            "(64, None, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## 모델 사용\n",
        "\n",
        "이제 모델을 실행하여 원하는대로 동작하는지 확인해보자.\n",
        "\n",
        "먼저 출력의 형태를 확인하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU",
        "outputId": "1200be90-0a92-45df-8ba1-4a3202a72167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"(배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "위 예제에서 입력의 시퀀스 길이는 100이지만 모델은 임의 시퀀스 길이의 입력도 사용 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "outputId": "fd95256d-b62b-460d-8cc4-5ef5b9458902",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (64, None, 1024)          6297600   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10319169 (39.36 MB)\n",
            "Trainable params: 10319169 (39.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## 모델 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "이 문제는 표준 분류 문제로 취급될 수 있습니다. 이전 RNN 상태와 이번 타임 스텝(time step)의 입력으로 다음 문자의 클래스를 예측한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Optimizer, Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "`tf.keras.losses.sparse_softmax_crossentropy` 를 사용해 label을 벡터로 바꾸지 않고 loss를 계산한다.\n",
        "\n",
        "이 모델은 로짓을 반환하기 때문에 `from_logits` 플래그를 설정해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# example_batch_loss = loss(target_example_batch, example_batch_predictions). ---> 오류 코드\n",
        "# print(\"예측 배열 크기(shape): \", example_batch_predictions.shape, \" # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")\n",
        "# print(\"Loss: \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "### 학습준비\n",
        "* `tf.keras.Model.compile` 메서드를 사용하여 훈련 절차를 설정\n",
        "* 기본 매개변수의 `tf.keras.optimizers.Adam`과 손실 함수를 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### 체크포인트 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "`tf.keras.callbacks.ModelCheckpoint`를 사용하여 훈련 중 체크포인트(checkpoint)가 저장되도록 설정한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# the save point\n",
        "if use_colab:\n",
        "    checkpoint_dir ='./drive/My Drive/train_ckpt/text_gen/exp1'\n",
        "    if not os.path.isdir(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "else:\n",
        "    checkpoint_dir = 'text_gen/exp1'\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 monitor='loss',\n",
        "                                                 mode='auto',\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### 훈련 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS=5 # 5~10 에폭정도"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll",
        "outputId": "bb05719f-26e9-4574-e1d8-364d4c8cbf52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " history = model.fit(dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - ETA: 0s - loss: 3.2344 - accuracy: 0.1719\n",
            "Epoch 1: loss improved from inf to 3.23436, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 31s 141ms/step - loss: 3.2344 - accuracy: 0.1719\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.4901 - accuracy: 0.3024\n",
            "Epoch 2: loss improved from 3.23436 to 2.49011, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 28s 147ms/step - loss: 2.4901 - accuracy: 0.3024\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.2782 - accuracy: 0.3485\n",
            "Epoch 3: loss improved from 2.49011 to 2.27820, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 2.2782 - accuracy: 0.3485\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.1581 - accuracy: 0.3800\n",
            "Epoch 4: loss improved from 2.27820 to 2.15814, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 2.1581 - accuracy: 0.3800\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.0594 - accuracy: 0.4048\n",
            "Epoch 5: loss improved from 2.15814 to 2.05943, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 26s 141ms/step - loss: 2.0594 - accuracy: 0.4048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wj7xQuZa56ap"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## 텍스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### 최근 체크포인트 복원"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "이 예측 단계에선 Batch size 1을 사용한다.\n",
        "\n",
        "* RNN 상태가 타임 스텝에서 타임 스텝으로 전달되는 방식이기 때문에 모델은 한 번 빌드된 고정 배치 크기만 허용\n",
        "* 다른 배치 크기로 모델을 실행하려면 모델을 다시 빌드하고 체크포인트에서 가중치를 복원해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LycQ-ot_jjyu"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(checkpoint_dir)\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71xa6jnYVrAN",
        "outputId": "b2764038-b893-45ac-e39f-9135a6f6c32d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (1, None, 1024)           6297600   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10319169 (39.36 MB)\n",
            "Trainable params: 10319169 (39.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### 예측 루프\n",
        "\n",
        "텍스트 생성:\n",
        "\n",
        "* 시작 문자열 선택과 순환 신경망 상태를 초기화하고 생성할 문자 수를 설정\n",
        "\n",
        "* 시작 문자열과 순환 신경망 상태를 사용하여 다음 문자의 예측 배열을 가져온다.\n",
        "\n",
        "* 다음, 범주형 배열을 사용하여 예측된 문자의 인덱스를 계산\n",
        "\n",
        "* 이 예측된 문자를 모델의 다음 입력으로 활용\n",
        "\n",
        "* 모델에 의해 리턴된 RNN 상태는 모델로 피드백되어 이제는 하나의 단어가 아닌 더 많은 컨텍스트를 갖추게 된다.\n",
        "\n",
        "* 다음 단어를 예측한 후 수정된 RNN 상태가 다시 모델로 피드백되어 이전에 예측된 단어에서 더 많은 컨텍스트를 얻으면서 학습하는 방식\n",
        "\n",
        "![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
        "\n",
        "* 생성된 텍스트를 보면 모델이 언제 대문자로 나타나고, 절을 만들고 셰익스피어와 유사한 어휘를 가져오는지 볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n",
        "\n",
        "  # 생성할 문자의 수\n",
        "  num_generate = 1000\n",
        "\n",
        "  # 시작 문자열을 숫자로 변환(벡터화)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 결과를 저장할 빈 문자열\n",
        "  text_generated = []\n",
        "\n",
        "  # 온도가 낮으면 더 예측 가능한 텍스트 생성\n",
        "  # 온도가 높으면 더 의외의 텍스트 생성 (불확실성)\n",
        "  # 최적의 세팅을 찾기 위한 실험\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 여기에서 배치 크기 == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # 배치 차원 제거\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 예측된 단어를 다음 입력으로 모델에 전달\n",
        "      # 이전 은닉 상태와 함께\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktovv0RFhrkn",
        "outputId": "440e8ef5-7d86-4680-bd48-a351d79a51c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO: RXWQOYIO\n",
            "NF&&RN::;3 wr by Forely ongrams: as himse hivey shepe tpiste\n",
            "Tullete of youg nith e uof.\n",
            "\n",
            "FwAMIO:\n",
            "Nus, fors, Butaroused of and my upistlle misercef\n",
            "In ane aspeld in o hourd.\n",
            "Het usterfing ham dound doplersst of a kand withre\n",
            "Es I dast is leepord a swolld:\n",
            "And dy masher hith of hit shald con thy\n",
            "Misg tan ateald thand upantsar as, ind thee\n",
            "Thow that in thain fild caish is knees\n",
            "Afmunt Coors thay duthers spore\n",
            "veact ouper you foredord hare me not ipoot tanou.\n",
            "\n",
            "BARYINs:\n",
            "Four swinkel ou shaud dents; wold ol the bostel.\n",
            "\n",
            "PECRITES:\n",
            "By puldon. Wellist hand viseder!\n",
            "\n",
            "PEPTANTIS:\n",
            "Ke,, kigk; thinge mepeor ut:\n",
            "Ourd m me lotkn, colest hos my unelo you orarlo,\n",
            "Ovay, I may enour separe, you hool to sas a ywerts as chingoue se no share ase mart.\n",
            "\n",
            "GLORUSES:\n",
            "\n",
            "RAK:\n",
            "IF Aven the hice bosereat:\n",
            "No nobe makelest comse andion.\n",
            "\n",
            "SUMUS:\n",
            "Mownon,\n",
            "Ay horit cons aljurd\n",
            "So veos mee-'s sefwly farifa lose.\n",
            "\n",
            "CIOY SARMECHUS:\n",
            "I prathery. 'e lorbeep tontes,\n",
            "Shich! are Cominicusef\n",
            "Qidet a wheit hay is domee.\n",
            "\n",
            "QUCEN\n"
          ]
        }
      ]
    }
  ]
}